{"cells":[{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\norig_data = pd.read_csv(\"../input/compresive_strength_concrete.csv\")\norig_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aa8b2370b8d8731f1a9e05d7a0c9cd0e0d59ca8a"},"cell_type":"code","source":"data = orig_data.copy()\ndata.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"472827098fac17a64f72e600e57499c4a54c4ac6"},"cell_type":"code","source":"data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aa6f8f8d0f881cac50eaa50e830b84d32defef8b"},"cell_type":"code","source":"data.describe().T","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c6666ad03bd44551cf20d3f33bd6c0f123de82df"},"cell_type":"markdown","source":"No missing values, all numeric"},{"metadata":{"trusted":true,"_uuid":"a10b688d62b5eaa65cb357f638a18202c7b75b45"},"cell_type":"code","source":"#Changing column headers to just keep component names\ndata.columns = [col[:col.find(\"(\")].strip() for col in data.columns]\ndata.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"77b1798dcc6caccd12e01020d128f9fd35ec38bf"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(data[data.columns[:-1]],\n                                                    data[[data.columns[-1]]],\n                                                    test_size = .2,\n                                                    random_state = 1)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"f9083a7d2bd7a0d2d770ddb665193de1679ddbda"},"cell_type":"markdown","source":"Rather than splitting training data further to validation set, we will perform cross validation in all our training models."},{"metadata":{"trusted":true,"_uuid":"6dbad691f10d6803a058bb90e64a4f8d45e089b1"},"cell_type":"code","source":"from pandas.plotting import scatter_matrix\nimport matplotlib.pyplot as plt\nsm = scatter_matrix(x_train, figsize=(15,15), diagonal = 'kde')\n#Changing label rotation\n[s.xaxis.label.set_rotation(45) for s in sm.reshape(-1)]\n[s.yaxis.label.set_rotation(45) for s in sm.reshape(-1)]\n#Changing offset for label\n[s.get_yaxis().set_label_coords(-0.5,0.5) for s in sm.reshape(-1)]\n#Hiding ticks\n[s.set_xticks(()) for s in sm.reshape(-1)]\n[s.set_yticks(()) for s in sm.reshape(-1)]\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ae6ccc32be866faa3655fa449b0ba67c7a174660"},"cell_type":"markdown","source":"No high correlation between any two features. Lets verify with Heatmap."},{"metadata":{"trusted":true,"_uuid":"f6b1af60afaaeb62c64979240dbbc069c7dc0b2c"},"cell_type":"code","source":"import seaborn as sns\nsns.heatmap(x_train.corr().abs())\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"51e073e69e209948b7234d2b6811bae60e173ff6"},"cell_type":"markdown","source":"Little correlation of ~0.6 between Superplasticizer and Water (which is negative as evident from scatter matrix), but lets move forward as is."},{"metadata":{"trusted":true,"_uuid":"1c1907b7b5aa877cf4c44b3a455e7a9099b41f44"},"cell_type":"code","source":"#Scaling the features\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler().fit(x_train)\nx_train_scaled = pd.DataFrame(scaler.transform(x_train),\n                              columns = x_train.columns)\nx_train_scaled.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"32a633109a81af0fd9be080b7f3dbd5f4d6a767a"},"cell_type":"code","source":"#We will save the model performance metrics in a DataFrame\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import KFold, cross_val_score\nimport numpy as np\nModel = []\nRMSE = []\nR_sq = []\ncv = KFold(5, random_state = 1)\n\n#Creating a Function to append the cross validation scores of the algorithms\ndef input_scores(name, model, x, y):\n    Model.append(name)\n    RMSE.append(np.sqrt((-1) * cross_val_score(model, x, y, cv=cv, \n                                               scoring='neg_mean_squared_error').mean()))\n    R_sq.append(cross_val_score(model, x, y, cv=cv, scoring='r2').mean())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"28f664b6677e3ca89bebd2ef0e20ea6010dfab07"},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import (RandomForestRegressor, GradientBoostingRegressor, \n                              AdaBoostRegressor)\n\nnames = ['Linear Regression', 'Ridge Regression', 'Lasso Regression',\n         'K Neighbors Regressor', 'Decision Tree Regressor', \n         'Random Forest Regressor', 'Gradient Boosting Regressor',\n         'Adaboost Regressor']\nmodels = [LinearRegression(), Ridge(), Lasso(),\n          KNeighborsRegressor(), DecisionTreeRegressor(),\n          RandomForestRegressor(), GradientBoostingRegressor(), \n          AdaBoostRegressor()]\n\n#Running all algorithms\nfor name, model in zip(names, models):\n    input_scores(name, model, x_train_scaled, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e07bfa1dc0cd6618f2e1629cc12b02a7d37ff021"},"cell_type":"code","source":"evaluation = pd.DataFrame({'Model': Model,\n                           'RMSE': RMSE,\n                           'R Squared': R_sq})\nprint(\"FOLLOWING ARE THE TRAINING SCORES: \")\nevaluation","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"558d9e4f17d037c19cd55a29cb670fdac42d3e0e"},"cell_type":"markdown","source":"Gradient Boosting Regressor has the lowest RMSE, highest R-Squared"},{"metadata":{"trusted":true,"_uuid":"fb97363091c2d342e8d818309243bc79ef186094"},"cell_type":"code","source":"#tuning this base model\nGradientBoostingRegressor()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"73a4b2429b3699c74b4da3408991fcc31d6ed0f1"},"cell_type":"code","source":"#tuning for number of trees\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = {'n_estimators':range(20,1001,10),\n              'max_depth':[10], #range(5,16,2), \n              'min_samples_split':[100], #range(200,1001,200), \n              'learning_rate':[0.2]}\nclf = GridSearchCV(GradientBoostingRegressor(random_state=1), \n                   param_grid = param_grid, scoring='r2', \n                   cv=cv).fit(x_train_scaled, y_train)\nprint(clf.best_estimator_) \nprint(\"R Squared:\",clf.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6157471242ab7dbfd4e3af06237338ba3fbb4792"},"cell_type":"code","source":"#tuning the tree specific parameters\nparam_grid = {'n_estimators': [230],\n              'max_depth': range(10,31,2), \n              'min_samples_split': range(50,501,10), \n              'learning_rate':[0.2]}\nclf = GridSearchCV(GradientBoostingRegressor(random_state=1), \n                   param_grid = param_grid, scoring='r2', \n                   cv=cv).fit(x_train_scaled, y_train)\nprint(clf.best_estimator_) \nprint(\"R Squared:\",clf.best_score_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1da11007e9b2f1e54f8d6300024a5955b0924403"},"cell_type":"code","source":"#now increasing number of trees and decreasing learning rate proportionally\nclf = GradientBoostingRegressor(random_state=1, max_depth=20, \n                                min_samples_split=170, n_estimators=230*2, \n                                learning_rate=0.2/2)\nprint(\"R Squared:\",cross_val_score(clf, x_train_scaled, y_train, cv=cv, scoring='r2').mean())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"918d923de8ccf6945db3c61ed68a915be59233ff"},"cell_type":"markdown","source":"**Since score improved, the best model is GradientBoostingRegressor with learning_rate= 0.2/2, max_depth= 20, min_samples_split= 170, n_estimators= 230*2**"},{"metadata":{"trusted":true,"_uuid":"432dcb20a9f0a4b9c9f2e81ef7a93da8d42f72fc"},"cell_type":"code","source":"#applying this model on test data\nx_test_scaled = pd.DataFrame(scaler.transform(x_test),\n                             columns = x_test.columns)\nclf = GradientBoostingRegressor(learning_rate=0.2/2, max_depth=20,\n                                min_samples_split=170, n_estimators=230*2, \n                                random_state=1).fit(x_train_scaled, y_train)\nprint(\"Test RMSE: \", np.sqrt(mean_squared_error(y_test, clf.predict(x_test_scaled))))\nprint(\"Test R^2: \", r2_score(y_test, clf.predict(x_test_scaled)))","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}